Notebooks : 
-- https://www.kaggle.com/ash316/eda-to-prediction-dietanic

1. Figure out if the problem is classification / regression, supervised / unsupervised, multiclass / multilabel 
2. For classification problem make sure to use Binning(preferred) / Normalistion to reduce continuous features to categorical.

Missing Values:
1. sklearn imputer to fill NaN with Mean Median Mode
2. My way: if feature is too important then regress missing values ie predict it given other variable
3. If it is a categorical variable, the default value is assigned. The missing value is assigned a default value.
4. If you have a distribution of data coming, for normal distribution give the mean value.
5. If 80% of the values for a variable are missing then you can answer that you would be dropping the variable instead of treating the missing values.
6. Use plotted graphs to know how to fill missing values

---------------------------------------------------------------------------------------------------------------------------
Data Visualisation:
PLOT EVERYTHING THAT COMES TO YOUR MIND 
QUARTILES - Q1 - middle value between smallest and median, Q2 - median, Q3 - middle value between median and highest value
[smallest, Q1, Q2, Q3, Highest]
FActor plots boxplots
 
Plot all categorical variables vs frequencies
Plot all categorical features with respect to their classes and see which of them are dominant
sns.factorplot('Pclass','Survived',hue='Sex',data=data)
plt.show()
We use FactorPlot in this case, because they make the seperation of categorical values easy.
Looking at the CrossTab and the FactorPlot, we can easily infer that survival for Women from Pclass1 is about 95-96%, as only 3 out of 94 Women from Pclass1 died.
It is evident that irrespective of Pclass, Women were given first priority while rescue. Even Men from Pclass1 have a very low survival rate.
Looks like Pclass is also an important feature. Lets analyse other features.
-----------------------------------------------------------

But the problem is, there were many people with many different ages. We just cant assign a 4 year kid with the mean age that is 29 years. Is there any way to 
find out what age-band does the passenger lie??
Bingo!!!!, we can check the Name feature. Looking upon the feature, we can see that the names have a salutation like Mr or Mrs. Thus we can assign the mean 
values of Mr and Mrs to the respective groups.
''What's In A Name??''---> Feature :p

Problem With Age Feature:
As I have mentioned earlier that Age is a continous feature, there is a problem with Continous Variables in Machine Learning Models.
Eg:If I say to group or arrange Sports Person by Sex, We can easily segregate them by Male and Female.
Now if I say to group them by their Age, then how would you do it? If there are 30 Persons, there may be 30 age values. Now this is problematic.
We need to convert these continous values into categorical values by either Binning or Normalisation. I will be using binning i.e group a range of ages into a 
single bin or assign them a single value.
Okay so the maximum age of a passenger was 80. So lets divide the range from 0-80 into 5 bins. So 80/5=16. So bins of size 16.


For Continuous Variable:
skewness - tail being biased towards one end.
kurtosis - fatness of tails hampers confidence intervals. data sets with high kurtosis tend to have heavy tails, or outliers.

Outlier Detection and Removal: 
If it lies outside 99% confindence interval, ir if variance very high from mean.
The Z-score is the signed number of standard deviations by which the value of an observation or data point is above or below the mean value of what is being observed 
or measured.


Convert a non-stationary process to a stationary process 

Variance inflation factor to detect multicollinearity:
In statistics, multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted 
from the others with a substantial degree of accuracy.
Methods:
1. Check the correlation table but its just bivariate
2. for multivariate, run auxiliary regressions ie try to predict first independent variable with the help of all others then second with elp of all others and so on
measure R2 for all a high variable might indicate depence. Or better way is to calculate VIF = 1 / 1-R2 if >5 then dont include that variable in multiple regression.

SPEARMAN AND PEARSON CORRELATIONS and corresponding heat maps

Plot correlation matrix to reduce the number of features use PCA where the number of components = Total features - redundant features or use VIF


Last. Scaling and splitting the data

Random forrest/XGBoost as a Forward Feature Selector 


